package.json
json{
  "name": "task-management-api",
  "version": "1.0.0",
  "description": "Task Management API with CI/CD pipeline",
  "main": "src/app.js",
  "scripts": {
    "start": "node src/app.js",
    "dev": "nodemon src/app.js",
    "test": "jest --coverage",
    "test:unit": "jest tests/unit --coverage",
    "test:integration": "jest tests/integration",
    "test:e2e": "jest tests/e2e",
    "lint": "eslint src/ tests/",
    "lint:fix": "eslint src/ tests/ --fix",
    "security": "npm audit && snyk test",
    "build": "docker build -t task-api .",
    "docker:dev": "docker-compose up -d"
  },
  "dependencies": {
    "express": "^4.18.2",
    "mongoose": "^7.0.3",
    "jsonwebtoken": "^9.0.0",
    "bcryptjs": "^2.4.3",
    "helmet": "^6.1.5",
    "cors": "^2.8.5",
    "dotenv": "^16.0.3",
    "express-rate-limit": "^6.7.0",
    "joi": "^17.9.1"
  },
  "devDependencies": {
    "jest": "^29.5.0",
    "supertest": "^6.3.3",
    "nodemon": "^2.0.22",
    "eslint": "^8.39.0",
    "snyk": "^1.1143.0"
  }
}
src/app.js (Main Application)
javascriptconst express = require('express');
const mongoose = require('mongoose');
const helmet = require('helmet');
const cors = require('cors');
const rateLimit = require('express-rate-limit');
require('dotenv').config();

const taskRoutes = require('./routes/tasks');
const userRoutes = require('./routes/users');

const app = express();
const PORT = process.env.PORT || 3000;

// Security middleware
app.use(helmet());
app.use(cors());

// Rate limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100 // limit each IP to 100 requests per windowMs
});
app.use(limiter);

// Body parsing middleware
app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true }));

// Database connection
mongoose.connect(process.env.MONGODB_URI || 'mongodb://localhost:27017/taskapi', {
  useNewUrlParser: true,
  useUnifiedTopology: true,
});

// Health check endpoint
app.get('/health', (req, res) => {
  res.status(200).json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    version: process.env.APP_VERSION || '1.0.0'
  });
});

// API routes
app.use('/api/tasks', taskRoutes);
app.use('/api/users', userRoutes);

// Error handling middleware
app.use((err, req, res, next) => {
  console.error(err.stack);
  res.status(500).json({ error: 'Something went wrong!' });
});

// 404 handler
app.use('*', (req, res) => {
  res.status(404).json({ error: 'Route not found' });
});

const server = app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});

module.exports = { app, server };
src/controllers/taskController.js
javascriptconst Task = require('../models/Task');

const taskController = {
  async getAllTasks(req, res) {
    try {
      const tasks = await Task.find({ userId: req.user.id });
      res.json(tasks);
    } catch (error) {
      res.status(500).json({ error: error.message });
    }
  },

  async createTask(req, res) {
    try {
      const task = new Task({
        ...req.body,
        userId: req.user.id
      });
      await task.save();
      res.status(201).json(task);
    } catch (error) {
      res.status(400).json({ error: error.message });
    }
  },

  async updateTask(req, res) {
    try {
      const task = await Task.findOneAndUpdate(
        { _id: req.params.id, userId: req.user.id },
        req.body,
        { new: true }
      );
      if (!task) {
        return res.status(404).json({ error: 'Task not found' });
      }
      res.json(task);
    } catch (error) {
      res.status(400).json({ error: error.message });
    }
  },

  async deleteTask(req, res) {
    try {
      const task = await Task.findOneAndDelete({
        _id: req.params.id,
        userId: req.user.id
      });
      if (!task) {
        return res.status(404).json({ error: 'Task not found' });
      }
      res.status(204).send();
    } catch (error) {
      res.status(400).json({ error: error.message });
    }
  }
};

module.exports = taskController;
2. Testing Strategy
tests/unit/controllers/taskController.test.js
javascriptconst taskController = require('../../../src/controllers/taskController');
const Task = require('../../../src/models/Task');

jest.mock('../../../src/models/Task');

describe('Task Controller', () => {
  let req, res;

  beforeEach(() => {
    req = {
      user: { id: 'user123' },
      body: {},
      params: {}
    };
    res = {
      json: jest.fn(),
      status: jest.fn().mockReturnThis(),
      send: jest.fn()
    };
  });

  describe('getAllTasks', () => {
    it('should return all tasks for user', async () => {
      const mockTasks = [
        { _id: '1', title: 'Task 1', userId: 'user123' },
        { _id: '2', title: 'Task 2', userId: 'user123' }
      ];
      Task.find.mockResolvedValue(mockTasks);

      await taskController.getAllTasks(req, res);

      expect(Task.find).toHaveBeenCalledWith({ userId: 'user123' });
      expect(res.json).toHaveBeenCalledWith(mockTasks);
    });

    it('should handle errors', async () => {
      Task.find.mockRejectedValue(new Error('Database error'));

      await taskController.getAllTasks(req, res);

      expect(res.status).toHaveBeenCalledWith(500);
      expect(res.json).toHaveBeenCalledWith({ error: 'Database error' });
    });
  });
});
tests/integration/api/tasks.test.js
javascriptconst request = require('supertest');
const { app } = require('../../../src/app');
const mongoose = require('mongoose');
const jwt = require('jsonwebtoken');

describe('Tasks API Integration Tests', () => {
  let authToken;

  beforeAll(async () => {
    // Connect to test database
    await mongoose.connect(process.env.TEST_MONGODB_URI);
    
    // Create auth token for testing
    authToken = jwt.sign({ id: 'testuser' }, process.env.JWT_SECRET);
  });

  afterAll(async () => {
    await mongoose.connection.close();
  });

  beforeEach(async () => {
    // Clean up database before each test
    await mongoose.connection.db.dropDatabase();
  });

  describe('GET /api/tasks', () => {
    it('should return empty array for new user', async () => {
      const response = await request(app)
        .get('/api/tasks')
        .set('Authorization', `Bearer ${authToken}`)
        .expect(200);

      expect(response.body).toEqual([]);
    });
  });

  describe('POST /api/tasks', () => {
    it('should create a new task', async () => {
      const taskData = {
        title: 'Test Task',
        description: 'Test Description',
        priority: 'high'
      };

      const response = await request(app)
        .post('/api/tasks')
        .set('Authorization', `Bearer ${authToken}`)
        .send(taskData)
        .expect(201);

      expect(response.body.title).toBe(taskData.title);
      expect(response.body.description).toBe(taskData.description);
    });
  });
});
3. Docker Configuration
Dockerfile (Multi-stage build)
dockerfile# Build stage
FROM node:18-alpine AS builder

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production && npm cache clean --force

# Production stage
FROM node:18-alpine AS production

# Create non-root user
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

WORKDIR /app

# Copy built dependencies
COPY --from=builder /app/node_modules ./node_modules

# Copy application code
COPY src/ ./src/
COPY package*.json ./

# Change ownership to nodejs user
RUN chown -R nodejs:nodejs /app

# Switch to non-root user
USER nodejs

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3000/health', (res) => { process.exit(res.statusCode === 200 ? 0 : 1) })"

EXPOSE 3000

CMD ["npm", "start"]
docker-compose.yml (Development)
yamlversion: '3.8'

services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - MONGODB_URI=mongodb://mongo:27017/taskapi
      - JWT_SECRET=dev-secret-key
    depends_on:
      - mongo
    volumes:
      - ./src:/app/src
      - ./tests:/app/tests
    command: npm run dev

  mongo:
    image: mongo:6.0
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    environment:
      - MONGO_INITDB_DATABASE=taskapi

  mongo-express:
    image: mongo-express
    ports:
      - "8081:8081"
    environment:
      - ME_CONFIG_MONGODB_SERVER=mongo
      - ME_CONFIG_MONGODB_PORT=27017
    depends_on:
      - mongo

volumes:
  mongo_data:
4. CI/CD Pipeline (GitHub Actions)
.github/workflows/ci.yml (Continuous Integration)
yamlname: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  NODE_VERSION: '18'
  MONGODB_URI: mongodb://localhost:27017/test_taskapi

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      mongodb:
        image: mongo:6.0
        ports:
          - 27017:27017
        options: >-
          --health-cmd "mongosh --eval 'db.adminCommand(\"ping\")'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Run linting
      run: npm run lint

    - name: Run unit tests
      run: npm run test:unit

    - name: Run integration tests
      run: npm run test:integration
      env:
        TEST_MONGODB_URI: ${{ env.MONGODB_URI }}
        JWT_SECRET: test-jwt-secret

    - name: Run security audit
      run: npm run security

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage/lcov.info
        flags: unittests
        name: codecov-umbrella

  build:
    needs: test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Login to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKER_USERNAME }}/task-api:latest
          ${{ secrets.DOCKER_USERNAME }}/task-api:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  security-scan:
    needs: build
    runs-on: ubuntu-latest
    
    steps:
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ secrets.DOCKER_USERNAME }}/task-api:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
.github/workflows/cd-staging.yml (Continuous Deployment - Staging)
yamlname: Deploy to Staging

on:
  workflow_run:
    workflows: ["CI Pipeline"]
    branches: [develop]
    types: [completed]

env:
  KUBE_NAMESPACE: staging
  APP_NAME: task-api

jobs:
  deploy-staging:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG }}

    - name: Deploy to staging
      run: |
        # Update image tag in deployment
        sed -i "s|IMAGE_TAG|${{ github.sha }}|g" k8s/deployment.yaml
        
        # Apply Kubernetes manifests
        kubectl apply -f k8s/ -n ${{ env.KUBE_NAMESPACE }}
        
        # Wait for deployment to complete
        kubectl rollout status deployment/${{ env.APP_NAME }} -n ${{ env.KUBE_NAMESPACE }}

    - name: Run smoke tests
      run: |
        # Wait for service to be ready
        sleep 30
        
        # Get service URL
        SERVICE_URL=$(kubectl get service ${{ env.APP_NAME }} -n ${{ env.KUBE_NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
        
        # Run health check
        curl -f http://$SERVICE_URL/health || exit 1
        
        # Run basic API tests
        npm run test:e2e
      env:
        API_BASE_URL: http://$SERVICE_URL

    - name: Notify deployment status
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        text: "Staging deployment ${{ job.status }} for commit ${{ github.sha }}"
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
.github/workflows/cd-production.yml (Continuous Deployment - Production)
yamlname: Deploy to Production

on:
  push:
    tags:
      - 'v*'

env:
  KUBE_NAMESPACE: production
  APP_NAME: task-api

jobs:
  deploy-production:
    runs-on: ubuntu-latest
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG }}

    - name: Blue-Green Deployment
      run: |
        # Get current deployment color
        CURRENT_COLOR=$(kubectl get deployment ${{ env.APP_NAME }}-blue -n ${{ env.KUBE_NAMESPACE }} -o jsonpath='{.metadata.labels.active}' 2>/dev/null || echo "false")
        
        if [ "$CURRENT_COLOR" = "true" ]; then
          NEW_COLOR="green"
          OLD_COLOR="blue"
        else
          NEW_COLOR="blue"
          OLD_COLOR="green"
        fi
        
        echo "Deploying to $NEW_COLOR environment"
        
        # Update deployment with new image
        sed -i "s|IMAGE_TAG|${{ github.ref_name }}|g" k8s/deployment-$NEW_COLOR.yaml
        kubectl apply -f k8s/deployment-$NEW_COLOR.yaml -n ${{ env.KUBE_NAMESPACE }}
        
        # Wait for new deployment
        kubectl rollout status deployment/${{ env.APP_NAME }}-$NEW_COLOR -n ${{ env.KUBE_NAMESPACE }}
        
        # Run health checks on new deployment
        kubectl wait --for=condition=available deployment/${{ env.APP_NAME }}-$NEW_COLOR -n ${{ env.KUBE_NAMESPACE }} --timeout=300s
        
        # Switch traffic to new deployment
        kubectl patch service ${{ env.APP_NAME }} -n ${{ env.KUBE_NAMESPACE }} -p '{"spec":{"selector":{"version":"'$NEW_COLOR'"}}}'
        
        # Mark new deployment as active
        kubectl label deployment ${{ env.APP_NAME }}-$NEW_COLOR active=true -n ${{ env.KUBE_NAMESPACE }} --overwrite
        kubectl label deployment ${{ env.APP_NAME }}-$OLD_COLOR active=false -n ${{ env.KUBE_NAMESPACE }} --overwrite
        
        echo "Successfully deployed to production ($NEW_COLOR)"

    - name: Post-deployment tests
      run: |
        # Wait for DNS propagation
        sleep 60
        
        # Run comprehensive tests
        npm run test:e2e
      env:
        API_BASE_URL: https://api.taskmanager.com

    - name: Cleanup old deployment
      run: |
        # Keep old deployment for quick rollback (scale to 0)
        OLD_COLOR=$(kubectl get deployment -l "app=${{ env.APP_NAME }},active=false" -n ${{ env.KUBE_NAMESPACE }} -o jsonpath='{.items[0].metadata.labels.version}')
        kubectl scale deployment ${{ env.APP_NAME }}-$OLD_COLOR --replicas=0 -n ${{ env.KUBE_NAMESPACE }}
5. Kubernetes Manifests
k8s/deployment.yaml
yamlapiVersion: apps/v1
kind: Deployment
metadata:
  name: task-api
  labels:
    app: task-api
    version: blue
    active: "true"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: task-api
      version: blue
  template:
    metadata:
      labels:
        app: task-api
        version: blue
    spec:
      containers:
      - name: task-api
        image: username/task-api:IMAGE_TAG
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          value: "production"
        - name: MONGODB_URI
          valueFrom:
            secretKeyRef:
              name: task-api-secrets
              key: mongodb-uri
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: task-api-secrets
              key: jwt-secret
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
      imagePullSecrets:
      - name: docker-registry-secret
k8s/service.yaml
yamlapiVersion: v1
kind: Service
metadata:
  name: task-api
spec:
  selector:
    app: task-api
    version: blue
  ports:
  - protocol: TCP
    port: 80
    targetPort: 3000
  type: LoadBalancer
6. Monitoring & Observability
scripts/health-check.sh
bash#!/bin/bash

API_URL="https://api.taskmanager.com"
WEBHOOK_URL="${SLACK_WEBHOOK_URL}"

# Function to send alert
send_alert() {
    local message="$1"
    local color="$2"
    
    curl -X POST -H 'Content-type: application/json' \
        --data "{\"attachments\":[{\"color\":\"$color\",\"text\":\"$message\"}]}" \
        "$WEBHOOK_URL"
}

# Health check
response=$(curl -s -o /dev/null -w "%{http_code}" "$API_URL/health")

if [ "$response" != "200" ]; then
    send_alert "🚨 API Health Check Failed! Status: $response" "danger"
    exit 1
else
    echo "✅ Health check passed"
fi

# Performance check
response_time=$(curl -s -o /dev/null -w "%{time_total}" "$API_URL/health")
threshold=2.0

if (( $(echo "$response_time > $threshold" | bc -l) )); then
    send_alert "⚠️ API Response Time High: ${response_time}s (threshold: ${threshold}s)" "warning"
fi

echo "Response time: ${response_time}s"
scripts/rollback.sh
bash#!/bin/bash

NAMESPACE="production"
APP_NAME="task-api"

echo "🔄 Starting rollback process..."

# Get current active deployment
CURRENT_ACTIVE=$(kubectl get deployment -l "app=$APP_NAME,active=true" -n $NAMESPACE -o jsonpath='{.items[0].metadata.labels.version}')
echo "Current active deployment: $CURRENT_ACTIVE"

# Determine rollback target
if [ "$CURRENT_ACTIVE" = "blue" ]; then
    ROLLBACK_TARGET="green"
else
    ROLLBACK_TARGET="blue"
fi

echo "Rolling back to: $ROLLBACK_TARGET"

# Scale up rollback target
kubectl scale deployment $APP_NAME-$ROLLBACK_TARGET --replicas=3 -n $NAMESPACE

# Wait for rollback deployment to be ready
kubectl wait --for=condition=available deployment/$APP_NAME-$ROLLBACK_TARGET -n $NAMESPACE --timeout=300s

# Switch traffic
kubectl patch service $APP_NAME -n $NAMESPACE -p '{"spec":{"selector":{"version":"'$ROLLBACK_TARGET'"}}}'

# Update labels
kubectl label deployment $APP_NAME-$ROLLBACK_TARGET active=true -n $NAMESPACE --overwrite
kubectl label deployment $APP_NAME-$CURRENT_ACTIVE active=false -n $NAMESPACE --overwrite

# Scale down old deployment
kubectl scale deployment $APP_NAME-$CURRENT_ACTIVE --replicas=0 -n $NAMESPACE

echo "✅ Rollback completed successfully!"

# Send notification
curl -X POST -H 'Content-type: application/json' \
    --data '{"text":"🔄 Production rollback completed: '$CURRENT_ACTIVE' → '$ROLLBACK_TARGET'"}' \
    "$SLACK_WEBHOOK_URL"
7. Infrastructure as Code (Terraform)
terraform/main.tf
hclprovider "aws" {
  region = var.aws_region
}

# EKS Cluster
module "eks" {
  source = "terraform-aws-modules/eks/aws"
  
  cluster_name    = var.cluster_name
  cluster_version = "1.24"
  
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets
  
  node_groups = {
    main = {
      desired_capacity = 2
      max_capacity     = 4
      min_capacity     = 1
      
      instance_types = ["t3.medium"]
      
      k8s_labels = {
        Environment = var.environment
        Application = "task-api"
      }
    }
  }
}

# RDS Database
resource "aws_db_instance" "mongodb" {
  identifier = "${var.cluster_name}-mongodb"
  
  engine         = "mongodb"
  engine_version = "6.0"
  instance_class = "db.t3.micro"
  
  allocated_storage = 20
  storage_encrypted = true
  
  db_name  = "taskapi"
  username = var.db_username
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  tags = {
    Name        = "${var.cluster_name}-mongodb"
    Environment = var.environment
  }
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "${var.cluster_name}-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets           = module.vpc.public_subnets
  
  enable_deletion_protection = false
  
  tags = {
    Environment = var.environment
  }
}
